{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cc173b3",
   "metadata": {},
   "source": [
    "Generate a random float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eac9f11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/himeshi/.cache/torch_extensions/py310_cu126 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/himeshi/.cache/torch_extensions/py310_cu126/quant_cpu/build.ninja...\n",
      "Building extension module quant_cpu...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/4] c++ -MMD -MF sim_helper.o.d -DTORCH_EXTENSION_NAME=quant_cpu -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -isystem /home/himeshi/conga25/conga25env/lib/python3.10/site-packages/torch/include -isystem /home/himeshi/conga25/conga25env/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/himeshi/.pyenv/versions/3.10.4/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -O3 -std=c++17 -fPIC -c /home/himeshi/conga25/QPyTorch/qtorch/quant/quant_cpu/sim_helper.cpp -o sim_helper.o \n",
      "[2/4] c++ -MMD -MF bit_helper.o.d -DTORCH_EXTENSION_NAME=quant_cpu -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -isystem /home/himeshi/conga25/conga25env/lib/python3.10/site-packages/torch/include -isystem /home/himeshi/conga25/conga25env/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/himeshi/.pyenv/versions/3.10.4/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -O3 -std=c++17 -fPIC -c /home/himeshi/conga25/QPyTorch/qtorch/quant/quant_cpu/bit_helper.cpp -o bit_helper.o \n",
      "[3/4] c++ -MMD -MF quant_cpu.o.d -DTORCH_EXTENSION_NAME=quant_cpu -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -isystem /home/himeshi/conga25/conga25env/lib/python3.10/site-packages/torch/include -isystem /home/himeshi/conga25/conga25env/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/himeshi/.pyenv/versions/3.10.4/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -O3 -std=c++17 -fPIC -c /home/himeshi/conga25/QPyTorch/qtorch/quant/quant_cpu/quant_cpu.cpp -o quant_cpu.o \n",
      "[4/4] c++ quant_cpu.o bit_helper.o sim_helper.o -shared -shared -L/home/himeshi/conga25/conga25env/lib/python3.10/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o quant_cpu.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module quant_cpu...\n",
      "Using /home/himeshi/.cache/torch_extensions/py310_cu126 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/himeshi/.cache/torch_extensions/py310_cu126/quant_cuda/build.ninja...\n",
      "/home/himeshi/conga25/conga25env/lib/python3.10/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module quant_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "715.5914759520085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module quant_cuda...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import struct\n",
    "import numpy as np\n",
    "import torch\n",
    "import qtorch\n",
    "from qtorch.quant import posit_quantize\n",
    "from qtorch.quant import convert_to_posit\n",
    "\n",
    "testfloat=random.random()*1e3\n",
    "print(testfloat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb25ead9",
   "metadata": {},
   "source": [
    "Extract sign, exponent and fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b15ff679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "Input float: 715.5914759520085\n",
      "Sign bit     : 0 (value: 1)\n",
      "Exponent bits: 10000001000 (value: 1032 => unbiased: 9)\n",
      "Fraction bits: 0110010111001011101101010111101111100111000111111010\n",
      "Fraction value: 1.3976396014687666\n",
      "Reconstructed value (approx): 715.5914759520085\n"
     ]
    }
   ],
   "source": [
    "def float_to_bits(f):\n",
    "    # Pack float into 8 bytes, then unpack as 64-bit unsigned int\n",
    "    packed = struct.pack('>d', f)  # Big endian double\n",
    "    integer_representation = int.from_bytes(packed, byteorder='big')\n",
    "    return integer_representation\n",
    "\n",
    "def extract_components(f):\n",
    "    bits = float_to_bits(f)\n",
    "\n",
    "    # Extract sign (1 bit), exponent (11 bits), and fraction (52 bits)\n",
    "    sign = (bits >> 63) & 0x1\n",
    "    exponent = (bits >> 52) & 0x7FF\n",
    "    fraction = bits & ((1 << 52) - 1)\n",
    "\n",
    "    # Compute values\n",
    "    sign_val = (-1) ** sign\n",
    "    exponent_val = exponent - 1023  # Bias for double-precision is 1023\n",
    "    print(type(1 * (2 ** -(1 + 1))))\n",
    "    fraction_val = 1 + sum(\n",
    "        [((fraction & (1 << (52 - i))) >> (51 - i)) * (1 * (2 ** -(i + 1))) for i in range(52)]\n",
    "    ) if exponent != 0 else sum(\n",
    "        [((fraction & (1 << (52 - i))) >> (51 - i)) * 1 * (2 ** -(i + 1)) for i in range(52)]\n",
    "    )\n",
    "\n",
    "    # Format binary strings\n",
    "    sign_bits = f\"{sign:b}\"\n",
    "    exponent_bits = f\"{exponent:011b}\"\n",
    "    fraction_bits = f\"{fraction:052b}\"\n",
    "\n",
    "    print(f\"Input float: {f}\")\n",
    "    print(f\"Sign bit     : {sign_bits} (value: {sign_val})\")\n",
    "    print(f\"Exponent bits: {exponent_bits} (value: {exponent} => unbiased: {exponent_val})\")\n",
    "    print(f\"Fraction bits: {fraction_bits}\")\n",
    "    print(f\"Fraction value: {fraction_val}\")\n",
    "    print(f\"Reconstructed value (approx): {sign_val * (2 ** exponent_val) * fraction_val}\")\n",
    "\n",
    "extract_components(testfloat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed885598",
   "metadata": {},
   "source": [
    "Convert float value to posit and check sign, exponent, fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8dc6967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type UInt16 but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m float_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(testfloat, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(float_tensor, torch\u001b[38;5;241m.\u001b[39mTensor))\n\u001b[0;32m----> 3\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_posit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfloat_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(b)\n",
      "File \u001b[0;32m~/conga25/QPyTorch/qtorch/quant/quant_function.py:383\u001b[0m, in \u001b[0;36mconvert_to_posit\u001b[0;34m(x, nsize, es, scale)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx is not a single precision Floating Point Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m quant_module \u001b[38;5;241m=\u001b[39m get_module(x)\n\u001b[0;32m--> 383\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mquant_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_from_float_to_posit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type UInt16 but found Float"
     ]
    }
   ],
   "source": [
    "float_tensor = torch.tensor(testfloat, dtype=torch.float)\n",
    "print(isinstance(float_tensor, torch.Tensor))\n",
    "b = convert_to_posit(float_tensor, nsize=16, es=2)\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conga25env (3.10.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
